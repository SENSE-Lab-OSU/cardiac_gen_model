{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cerebral Cortex Synthetic ECG Generation from the WESAD data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run the following 2 cells ONLY if in Colab else skip them. They will install miniconda on Colab. Before running, first activate GPU by: Edit > Notebook settings > Hardware accelerator > GPU > Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!which python\n",
    "!python --version\n",
    "#Check if GPU is detected\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "# If in Colab, install conda/mamba using condacolab python package and \n",
    "# wait until kernel restarts after the installation\n",
    "if IN_COLAB:\n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install_miniconda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Start running from following cell after kernel restarts OR when running locally on linux without dependencies installed. Don't run the cells above this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check notebook dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "HAVE_CARDIOGEN = importlib.util.find_spec(\"CardioGen\") is not None\n",
    "\n",
    "if(not HAVE_CARDIOGEN):\n",
    "    if IN_COLAB: \n",
    "        print(\"\\nGetting CardioGen\")\n",
    "        !git clone https://github.com/SENSE-Lab-OSU/cardio_gen_model.git\n",
    "        !conda env update -n base -f ./cardio_gen_model/conda_requirements_linux.yml\n",
    "        !pip install ./cardio_gen_model\n",
    "    else:\n",
    "        raise SystemExit(\"Please install CardioGen from https://github.com/SENSE-Lab-OSU/cardio_gen_model.git\")\n",
    "else:\n",
    "    print(\"CardioGen found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mFlow and Cerebral-Cortex dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAVA_HOME_DEFINED = \"JAVA_HOME\" in os.environ\n",
    "SPARK_HOME_DEFINED = \"SPARK_HOME\" in os.environ\n",
    "PYSPARK_PYTHON_DEFINED = \"PYSPARK_PYTHON\" in os.environ\n",
    "PYSPARK_DRIVER_PYTHON_DEFINED = \"PYSPARK_DRIVER_PYTHON\" in os.environ\n",
    "print(JAVA_HOME_DEFINED,SPARK_HOME_DEFINED,PYSPARK_PYTHON_DEFINED,PYSPARK_DRIVER_PYTHON_DEFINED)\n",
    "        \n",
    "if not JAVA_HOME_DEFINED:\n",
    "    if IN_COLAB:\n",
    "        if not os.path.exists(\"/usr/lib/jvm/java-8-openjdk-amd64/\"): \n",
    "            print(\"\\nGetting Java 8 SDK\")\n",
    "            !sudo apt update\n",
    "            !apt-get install -y openjdk-8-jdk-headless \n",
    "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/\"\n",
    "    else:\n",
    "        raise SystemExit(\"Please install the Java8 SDK and set the JAVA_HOME environment variable\")\n",
    "else:\n",
    "    print(\"JAVA_HOME defined\")\n",
    "\n",
    "\n",
    "if not SPARK_HOME_DEFINED:\n",
    "    if IN_COLAB:\n",
    "        if not os.path.exists(\"/content/spark-3.1.2-bin-hadoop2.7/\"):\n",
    "            print(\"\\nGetting Apache Spark 3.1.2\")\n",
    "            !wget  https://mirrors.ocf.berkeley.edu/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "            !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "        os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7/\"\n",
    "    else:\n",
    "        raise SystemExit(\"Please install spark-3.1.2-bin-hadoop2.7 and set the SPARK_HOME environment variable\")\n",
    "else:\n",
    "    print(\"SPARK_HOME defined\")\n",
    "\n",
    "\n",
    "if not PYSPARK_PYTHON_DEFINED:\n",
    "    if IN_COLAB:\n",
    "        os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "    else:\n",
    "        raise SystemExit(\"Please set the PYSPARK_PYTHON environment variable to your desired Python version\")\n",
    "else:\n",
    "    print(\"PYSPARK_PYTHON defined\")\n",
    "            \n",
    "if not PYSPARK_DRIVER_PYTHON_DEFINED:\n",
    "    if IN_COLAB:\n",
    "        os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "    else:\n",
    "        raise SystemExit(\"Please set the PYSPARK_DRIVER_PYTHON environment variable to your desired Python version\")\n",
    "else:\n",
    "    print(\"PYSPARK_DRIVER_PYTHON defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Start running from following cell when running locally on linux with all dependencies installed. Don't run the cells above this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from mFlow.Utilities.utilities import getDataDir\n",
    "from cerebralcortex.kernel import Kernel\n",
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "get_study_names()\n",
    "#from cerebralcortex.core.datatypes.datastream import DataStream\n",
    "import requests, tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading WESAD data archive to ./data/pre-training/mFlo/compressed\\cc_data.tar.bz2 .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Done\n",
      "Extracting WESAD data archive to ./data/pre-training/mFlo/wesad_all\\cc_data ... Done\n"
     ]
    }
   ],
   "source": [
    "def wesad_data_download(data_size=\"all\"):\n",
    "    '''\n",
    "    Downloads WESAD data if needed\n",
    "    '''\n",
    "\n",
    "    if(data_size==\"all\"):\n",
    "        compressed_file=\"cc_data.tar.bz2\"\n",
    "    elif(data_size==\"small\"):\n",
    "        compressed_file=\"s2_data.tar.bz2\"\n",
    "\n",
    "    compressed_data_url=\"http://mhealth.md2k.org/images/datasets/%s\"%compressed_file\n",
    "    compressed_data_local_file = os.path.join(compressed_data_dir,compressed_file)\n",
    "\n",
    "    #Make all data directories\n",
    "    for d in [base_data_dir, compressed_data_dir, wesad_data_dir]:\n",
    "        if(not os.path.exists(d)):\n",
    "            os.mkdir(d)\n",
    "\n",
    "    #Check if cc data store exists:\n",
    "    if(not os.path.exists(wesad_cc_dir)):\n",
    "        \n",
    "        #Check if compressed cc version of the data set exists\n",
    "        if(not os.path.isfile(compressed_data_local_file)):\n",
    "            print(\"Downloading WESAD data archive to %s \"%compressed_data_local_file,end=\"\")\n",
    "            #Download the compressed data file\n",
    "            response = requests.get(compressed_data_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            with open(compressed_data_local_file , 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024*1024): \n",
    "                    f.write(chunk)\n",
    "                    print(\".\",end=\"\",flush=True)\n",
    "            print(\"\")\n",
    "            print(\"Done\")\n",
    "        \n",
    "        #Extract the data set\n",
    "        print(\"Extracting WESAD data archive to %s ...\"%wesad_cc_dir,end=\"\",flush=True)\n",
    "        tar = tarfile.open(compressed_data_local_file)\n",
    "        tar.extractall(wesad_data_dir)\n",
    "        tar.close()\n",
    "        print(\" Done\")\n",
    "        \n",
    "class_name='S2'\n",
    "data_size='all'\n",
    "\n",
    "base_data_dir       = './data/pre-training/CC_data/'#getDataDir()\n",
    "compressed_data_dir = os.path.join(base_data_dir,\"compressed\")\n",
    "wesad_data_dir      = os.path.join(base_data_dir,\"wesad_\"+data_size)\n",
    "wesad_cc_dir        = os.path.join(wesad_data_dir,\"cc_data\")\n",
    "os.makedirs(base_data_dir,exist_ok=True)\n",
    "\n",
    "#Download the data if needed\n",
    "if (not os.path.exists(wesad_cc_dir)):\n",
    "    wesad_data_download(data_size='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wesad.chest.acc', 'wesad.chest.ecg', 'wesad.chest.eda', 'wesad.chest.emg', 'wesad.chest.resp', 'wesad.chest.temp', 'wesad.label', 'wesad.quest', 'wesad.wrist.acc', 'wesad.wrist.bvp', 'wesad.wrist.eda', 'wesad.wrist.temp']\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o71.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\TEMPCO~1.005\\AppData\\Local\\Temp/ipykernel_6684/2258044272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Get all the ecg data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mCC_datastream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wesad.chest.ecg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\cerebralcortex\\kernel.py\u001b[0m in \u001b[0;36mget_stream\u001b[1;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m###########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\cerebralcortex\\core\\data_manager\\raw\\stream_handler.py\u001b[0m in \u001b[0;36mget_stream\u001b[1;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[1;31m#df = df.dropDuplicates(subset=['timestamp'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\cerebralcortex\\core\\data_manager\\raw\\filebased_storage.py\u001b[0m in \u001b[0;36mread_file\u001b[1;34m(self, stream_name, version, user_id)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_storage_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_gpu_5\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o71.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "#Create CC kernel to load WESAD data\n",
    "cc_configs = {\"nosql_storage\": \"filesystem\", \n",
    "          \"filesystem\":{\"filesystem_path\": wesad_cc_dir },\n",
    "          \"relational_storage\": \"sqlite\",\n",
    "          \"sqlite\": {\"file_path\": wesad_cc_dir },\n",
    "         }\n",
    "CC = Kernel(cc_configs=cc_configs, study_name=\"wesad\")\n",
    "CC.sqlContext.sql(\"set spark.sql.shuffle.partitions=1\")\n",
    "print(CC.list_streams())\n",
    "\n",
    "#Get all the ecg data\n",
    "CC_datastream_ecg = CC.get_stream(\"wesad.chest.ecg\")\n",
    "CC_datastream_stres = CC.get_stream(\"wesad.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CardioGen.lib.data import load_data_wesad as load_data\n",
    "\n",
    "def get_user_ecg_data(CC_datastream,P_ID=\"S2\"):\n",
    "    P_ID=P_ID.lower()\n",
    "    #Get user's ecg data\n",
    "    filtered_user_data = CC_datastream.filter_user(P_ID)\n",
    "    #filtered_user_data.show(3,truncate=False)\n",
    "    #data.show(3, truncate=False)\n",
    "    #Convert CC DataStream to numpy()\n",
    "    df=filtered_user_data.toPandas()\n",
    "    #df_mini=df.iloc[:1000,:]\n",
    "    ecg_from_CC=df['ecg'].values.astype(np.float32)\n",
    "    #Resample ecg from 700Hz to 100Hz since current generative model works at 100 Hz\n",
    "    #ecg_resamp=load_data.resample(ecg_from_CC.reshape(-1,1),700,1,7)\n",
    "    return ecg_from_CC\n",
    "\n",
    "def get_user_stres_data(CC_datastream,P_ID=\"S2\"):\n",
    "    P_ID=P_ID.lower()\n",
    "    #Get user's ecg data\n",
    "    filtered_user_data = CC_datastream.filter_user(P_ID)\n",
    "    #filtered_user_data.show(3,truncate=False)\n",
    "    #data.show(3, truncate=False)\n",
    "    #Convert CC DataStream to numpy()\n",
    "    df=filtered_user_data.toPandas()\n",
    "    #df_mini=df.iloc[:1000,:]\n",
    "    stres_from_CC=df['label'].values.astype(np.float32)\n",
    "    #Resample ecg from 700Hz to 100Hz since current generative model works at 100 Hz\n",
    "    return stres_from_CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ID_in, P_ID_out='S7','S15'\n",
    "\n",
    "#Get Data. This step can take a lot of time due to pyspark to numpy datatype conversion\n",
    "lenth=210000\n",
    "ecg_in = (get_user_ecg_data(CC_datastream_ecg,P_ID=P_ID_in))[-lenth:]\n",
    "stres_in = (get_user_stres_data(CC_datastream_stres,P_ID=P_ID_in))[-lenth:]\n",
    "ecg_out = (get_user_ecg_data(CC_datastream_ecg,P_ID=P_ID_out))[-lenth:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modulators import ECG_HRV_Morph_Modulator, ECG_Morph_Modulator\n",
    "#ckpt_path='data/post-training/'\n",
    "ckpt_path='./cardio_gen_model/data/post-training/'\n",
    "Fs_in=700#100\n",
    "Fs_out=Fs_ecg #Synthetic signal sampling freq\n",
    "Fs_final=Fs_in*1\n",
    "# ECG HRV+Morph modulation\n",
    "hrv_morph_mod=ECG_HRV_Morph_Modulator(P_ID_out=P_ID_out,\n",
    "                                path=ckpt_path,Fs_tacho=5,Fs_out=Fs_out)\n",
    "# Produce synthetic from S15 to S15 itself to check performance of models\n",
    "ecg_hrv_morph_mod_check,_=hrv_morph_mod(ecg_out,stres_in,Fs=Fs_in,\n",
    "                                        Fs_final=Fs_final)\n",
    "# Produce synthetic from S7 to S15\n",
    "ecg_hrv_morph_mod,_=hrv_morph_mod(ecg_in,stres_in,Fs=Fs_in,\n",
    "                                  Fs_final=Fs_final)\n",
    "plt.figure();plt.plot(ecg_hrv_morph_mod)\n",
    "\n",
    "# Analyze HRV and Morphological properties\n",
    "morph_features,hrv_features= hrv_morph_mod.analyse_signal(ecg_out.flatten()\n",
    "                            ,Fs=Fs_in,title_hrv=P_ID_out+'_out',\n",
    "                            title_morph=P_ID_out+'_out')\n",
    "morph_features,hrv_features= hrv_morph_mod.analyse_signal(ecg_hrv_morph_mod_check\n",
    "                            ,Fs=Fs_final,title_hrv=P_ID_out+'_hrv_morph_synth_check',\n",
    "                            title_morph=P_ID_out+'_hrv_morph_synth_check')\n",
    "\n",
    "morph_features,hrv_features= hrv_morph_mod.analyse_signal(ecg_in.flatten()\n",
    "                            ,Fs=Fs_in,title_hrv=P_ID_in+'_in',\n",
    "                            title_morph=P_ID_in+'_in')\n",
    "morph_features,hrv_features= hrv_morph_mod.analyse_signal(ecg_hrv_morph_mod\n",
    "                            ,Fs=Fs_final,title_hrv=P_ID_out+'_hrv_morph_synth',\n",
    "                            title_morph=P_ID_out+'_hrv_morph_synth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG Morph modulation\n",
    "morph_mod=ECG_Morph_Modulator(P_ID_out=P_ID_out,\n",
    "                              path=ckpt_path,Fs_tacho=5,Fs_out=Fs_out)\n",
    "ecg_morph_mod,_=morph_mod(ecg_in,stres_in,Fs=Fs_in,Fs_final=Fs_final)\n",
    "\n",
    "morph_features,hrv_features= morph_mod.analyse_signal(ecg_morph_mod\n",
    "                            ,Fs=Fs_final,title_hrv=P_ID_in+'_morph_synth',\n",
    "                            title_morph=P_ID_out+'_morph_synth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
